{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912cd8c6-d405-4dfe-8897-46108e6a6af7",
   "metadata": {},
   "source": [
    "# RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631b09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: An OpenAI API key must be set here for application initialization, even if not in use.\n",
    "# If you're not utilizing OpenAI models, assign a placeholder string (e.g., \"not_used\").\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"not_used\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d7d995-7beb-40b5-9a44-afd350b7d221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The wife of a rich man fell sick, and as she felt that her end\n",
      "was drawing near, she called her only\n"
     ]
    }
   ],
   "source": [
    "# Cinderella story defined in sample.txt\n",
    "with open(\"demo/sample.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d51ebd-5597-4fdd-8c37-32636395081b",
   "metadata": {},
   "source": [
    "1) **Building**: RAPTOR recursively embeds, clusters, and summarizes chunks of text to construct a tree with varying levels of summarization from the bottom up. You can create a tree from the text in 'sample.txt' using `RA.add_documents(text)`.\n",
    "\n",
    "2) **Querying**: At inference time, the RAPTOR model retrieves information from this tree, integrating data across lengthy documents at different abstraction levels. You can perform queries on the tree with `RA.answer_question`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f58830-9004-48a4-b50e-61a855511d24",
   "metadata": {},
   "source": [
    "### Building the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3753fcf9-0a8e-4ab3-bf3a-6be38ef6cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raptor import RetrievalAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e843edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:18:19,477 - Use pytorch device_name: mps\n",
      "2025-12-23 16:18:19,477 - Load pretrained SentenceTransformer: nomic-ai/modernbert-embed-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start initializing RetrievalAugmentation...\n",
      "Validating QA model...\n",
      "QA model not provided in config\n",
      "Validating embedding model...\n",
      "Embedding model not provided in config\n",
      "Validating summarization model...\n",
      "Summarization model not provided in config\n",
      "Setting TreeBuilderConfig...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:18:20,860 - Use pytorch device_name: mps\n",
      "2025-12-23 16:18:20,862 - Load pretrained SentenceTransformer: nomic-ai/modernbert-embed-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting TreeRetrieverConfig...\n",
      "Embedding model not provided, defaulting to SBertEmbeddingModel\n",
      "2 config done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "2025-12-23 16:18:39,151 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.DeepSeekSummarizationModel object at 0x3115106e0>\n",
      "            Embedding Models: {'SBERT': <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x311510830>}\n",
      "            Cluster Embedding Model: SBERT\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2025-12-23 16:18:39,152 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.DeepSeekSummarizationModel object at 0x3115106e0>\n",
      "            Embedding Models: {'SBERT': <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x311510830>}\n",
      "            Cluster Embedding Model: SBERT\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2025-12-23 16:18:39,152 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.DeepSeekSummarizationModel object at 0x3115106e0>\n",
      "            Embedding Models: {'SBERT': <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x311510830>}\n",
      "            Cluster Embedding Model: SBERT\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: SBERT\n",
      "            Embedding Model: <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x317791a90>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            QA Model: <raptor.QAModels.UnifiedQAModel object at 0x3179481a0>\n",
      "            Tree Builder Type: cluster\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "RA = RetrievalAugmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9123c9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:29:23,732 - Creating Leaf Nodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90236200e3c942df95710264d9233606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a17567e979e45e7a9726e38dad098c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93c9042d7a74658b40cae56012f66ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bec510b39124d66a49cea6c755c0b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f5e07c434a40eb822fe53d996f2c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324cf30634b247eea426e53c81b3867d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18adf0dcd6d14a4b9230fe2759eb2438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4921bd7ecd6843929f7bce93e1035480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a07bb88b66484ea7f7d0535a40fea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f5ef7d23fc4e65a469473b86901864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e149e12404474ca88b880af4cb87e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ea1aa57cbe4a86862668032b3700f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77353fed151d40d89617da9ed000959a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9501fe7335b4f06addde7dc84c6910d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b75e6ff4892402ca94e4159f0d009da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6711550613fb4612897ff118bbfc85df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8356daaa715847d884b5b08ca785d2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b5a04c1e6944c7ae8a570dac8be2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e229e2676fc94fa69dc13e4c18ec27e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe3d6b0a5ad4a2c92691cee4554a4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6350339b932f4fec976347dda16f2b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a965dd7c6bf9405ab3677b6d81e9a843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb375df0e7847509fc3ff51c66b91fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0093ed01e39e4a1baf0088c636ff9a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1e8b88bb21411980f608acc45ffac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505b1f2c7d644602905056f9e08f6f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2494fc50e9bb4c248003aab1e1a451f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849cc2749a074a83b370f2f7701ccadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d7bf76bbd241fcba8ba04c50b18248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3ee549fa5c44258a57de2c1bfa7d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9644cbcfb1e4a4c904749b6d32c9ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56dfe88248204ce49989df6f1fc53820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160fe9c5603e41a792a8e63bbab3f344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a6f8fcd70a43a7b97a0c96af623432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a46faec63f547cc859f82cc7c7ec450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:29:28,651 - Created 35 Leaf Embeddings\n",
      "2025-12-23 16:29:28,651 - Building All Nodes\n",
      "2025-12-23 16:29:28,652 - Using Cluster TreeBuilder\n",
      "2025-12-23 16:29:28,652 - Constructing Layer 0\n",
      "2025-12-23 16:29:34,609 - Summarization Length: 100\n",
      "2025-12-23 16:29:34,976 - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-23 16:29:39,768 - Node Texts Length: 1047, Summarized Text Length: 99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b14261d63245c8980fa13e1bd637c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:29:40,117 - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-23 16:29:44,785 - Node Texts Length: 960, Summarized Text Length: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc1d99b65b54d069ec3977a9a3eda33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:29:45,086 - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-23 16:29:50,033 - Node Texts Length: 407, Summarized Text Length: 102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05281de324b423290a0bce55b39e40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:29:50,417 - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-23 16:29:55,060 - Node Texts Length: 867, Summarized Text Length: 101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4a19e5b4fe40d4a65a6acd693453b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:29:55,276 - Constructing Layer 1\n",
      "2025-12-23 16:29:55,277 - Stopping Layer construction: Cannot Create More Layers. Total Layers in tree: 1\n",
      "2025-12-23 16:29:55,277 - Successfully initialized TreeRetriever with Config \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: SBERT\n",
      "            Embedding Model: <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x317791a90>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# construct the tree and corresponding retriever\n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f219d60a-1f0b-4cee-89eb-2ae026f13e63",
   "metadata": {},
   "source": [
    "### Querying from the tree\n",
    "\n",
    "```python\n",
    "question = # any question\n",
    "RA.answer_question(question)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b4037c5-ad5a-424b-80e4-a67b8e00773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:36:45,718 - Using collapsed_tree\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7f5a2dc50244a9a8d059080819336e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  remarried the woman with two beautiful but cruel daughters.\n"
     ]
    }
   ],
   "source": [
    "question = \"How did Cinderella reach her happy ending ?\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5be7e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:31:51,494 - Tree successfully saved to demo/cinderella_myself\n"
     ]
    }
   ],
   "source": [
    "# Save the tree by calling RA.save(\"path/to/save\")\n",
    "SAVE_PATH = \"demo/cinderella_myself\"\n",
    "RA.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e845de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:38:28,302 - Use pytorch device_name: mps\n",
      "2025-12-23 16:38:28,303 - Load pretrained SentenceTransformer: nomic-ai/modernbert-embed-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start initializing RetrievalAugmentation...\n",
      "Validating QA model...\n",
      "QA model not provided in config\n",
      "Validating embedding model...\n",
      "Embedding model not provided in config\n",
      "Validating summarization model...\n",
      "Summarization model not provided in config\n",
      "Setting TreeBuilderConfig...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:38:29,698 - Use pytorch device_name: mps\n",
      "2025-12-23 16:38:29,698 - Load pretrained SentenceTransformer: nomic-ai/modernbert-embed-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting TreeRetrieverConfig...\n",
      "Embedding model not provided, defaulting to SBertEmbeddingModel\n",
      "2 config done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 16:38:35,144 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.DeepSeekSummarizationModel object at 0x333c06b10>\n",
      "            Embedding Models: {'SBERT': <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x1065878a0>}\n",
      "            Cluster Embedding Model: SBERT\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2025-12-23 16:38:35,145 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.DeepSeekSummarizationModel object at 0x333c06b10>\n",
      "            Embedding Models: {'SBERT': <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x1065878a0>}\n",
      "            Cluster Embedding Model: SBERT\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2025-12-23 16:38:35,145 - Successfully initialized TreeRetriever with Config \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: SBERT\n",
      "            Embedding Model: <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x3334a7f00>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "2025-12-23 16:38:35,145 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <raptor.SummarizationModels.DeepSeekSummarizationModel object at 0x333c06b10>\n",
      "            Embedding Models: {'SBERT': <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x1065878a0>}\n",
      "            Cluster Embedding Model: SBERT\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: SBERT\n",
      "            Embedding Model: <raptor.EmbeddingModels.SBertEmbeddingModel object at 0x3334a7f00>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            QA Model: <raptor.QAModels.UnifiedQAModel object at 0x3580d6d70>\n",
      "            Tree Builder Type: cluster\n",
      "        \n",
      "2025-12-23 16:38:35,152 - Using collapsed_tree\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297b633ffd9545328d35ec0aa9530366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1078 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  remarried the woman with two beautiful but cruel daughters.\n"
     ]
    }
   ],
   "source": [
    "# load back the tree by passing it into RetrievalAugmentation\n",
    "\n",
    "RA = RetrievalAugmentation(tree=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = RA.answer_question(question=question)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ab6ea-1c79-4ed1-97de-1c2e39d6db2e",
   "metadata": {},
   "source": [
    "## Using other Open Source Models for Summarization/QA/Embeddings\n",
    "\n",
    "If you want to use other models such as Llama or Mistral, you can very easily define your own models and use them with RAPTOR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from raptor import (\n",
    "    BaseSummarizationModel,\n",
    "    BaseQAModel,\n",
    "    BaseEmbeddingModel,\n",
    "    RetrievalAugmentationConfig,\n",
    ")\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5cef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use the Gemma, you will need to authenticate with HuggingFace, Skip this step, if you have the model already downloaded\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "# You can define your own Summarization model by extending the base Summarization Class.\n",
    "class GEMMASummarizationModel(BaseSummarizationModel):\n",
    "    def __init__(self, model_name=\"google/gemma-2b-it\"):\n",
    "        # Initialize the tokenizer and the pipeline for the GEMMA model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.summarization_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=torch.device(\n",
    "                \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            ),  # Use \"cpu\" if CUDA is not available\n",
    "        )\n",
    "\n",
    "    def summarize(self, context, max_tokens=150):\n",
    "        # Format the prompt for summarization\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Write a summary of the following, including as many key details as possible: {context}:\",\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate the summary using the pipeline\n",
    "        outputs = self.summarization_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "\n",
    "        # Extracting and returning the generated summary\n",
    "        summary = outputs[0][\"generated_text\"].strip()\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEMMAQAModel(BaseQAModel):\n",
    "    def __init__(self, model_name=\"google/gemma-2b-it\"):\n",
    "        # Initialize the tokenizer and the pipeline for the model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_name,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        )\n",
    "\n",
    "    def answer_question(self, context, question):\n",
    "        # Apply the chat template for the context and question\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Given Context: {context} Give the best full answer amongst the option to question {question}\",\n",
    "            }\n",
    "        ]\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate the answer using the pipeline\n",
    "        outputs = self.qa_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "\n",
    "        # Extracting and returning the generated answer\n",
    "        answer = outputs[0][\"generated_text\"][len(prompt) :]\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class SBertEmbeddingModel(BaseEmbeddingModel):\n",
    "    def __init__(self, model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def create_embedding(self, text):\n",
    "        return self.model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255791ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAC = RetrievalAugmentationConfig(\n",
    "    summarization_model=GEMMASummarizationModel(),\n",
    "    qa_model=GEMMAQAModel(),\n",
    "    embedding_model=SBertEmbeddingModel(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee46f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RA = RetrievalAugmentation(config=RAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe05daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"demo/sample.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Cinderella reach her happy ending?\"\n",
    "\n",
    "answer = RA.answer_question(question=question)\n",
    "\n",
    "print(\"Answer: \", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
